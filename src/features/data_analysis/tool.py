# src/features/data_analysis/tool.py

import os, pandas as pd, traceback
from langchain_experimental.agents import create_pandas_dataframe_agent
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.runnables import RunnableLambda
from src.config import PRIMARY_MODEL_NAME
from dotenv import load_dotenv
import streamlit as st
from src.utils.errors import create_tool_error
from pydantic import BaseModel, Field
from langchain_core.tools import tool

load_dotenv()
google_api_key = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY"))

class DataAnalysisInput(BaseModel):
    query: str = Field(..., description="ë°ì´í„° ë¶„ì„ì„ ìœ„í•´ Pandas Agentì—ê²Œ ì „ë‹¬í•  ì§ˆë¬¸")
    store_id: str | None = Field(None, description="ë¶„ì„ì˜ ì¤‘ì‹¬ì´ ë˜ëŠ” íŠ¹ì • ê°€ë§¹ì ì˜ ID")

# ì •] @tool ë°ì½”ë ˆì´í„°ì™€ Pydantic ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ëª…í™•í•œ ì¸í„°í˜ì´ìŠ¤ ì •ì˜
@tool(args_schema=DataAnalysisInput)
def data_analysis_tool(query: str, store_id: str | None = None) -> str:
    """
    Pandas DataFrameì„ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë°ì´í„°ì— ëŒ€í•œ ë³µì¡í•œ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    íŠ¹ì • ê°€ë§¹ì  ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•  ê²½ìš° store_idë¥¼ í•¨ê»˜ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤.
    """
    print(f"--- ğŸ› ï¸ Tool: data_analysis_tool í˜¸ì¶œë¨ (ID: {store_id}) ---")
    data_dir = "./data/"
    try:
        # [ì‚­] stateì—ì„œ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” ë¡œì§ ì œê±°
        csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
        if not csv_files: return "ë¶„ì„í•  CSV íŒŒì¼ì´ 'data' í´ë”ì— ì—†ìŠµë‹ˆë‹¤."
        
        
        df_map = {} 
        for f in csv_files:
            file_path = os.path.join(data_dir, f)
            try: df = pd.read_csv(file_path, encoding='utf-8')
            except UnicodeDecodeError: df = pd.read_csv(file_path, encoding='cp949')
            df_map[f] = df
        
        dataframes = list(df_map.values())

    except Exception as e:
        return create_tool_error("data_analyzer", e)

    llm = ChatGoogleGenerativeAI(model=PRIMARY_MODEL_NAME, google_api_key=google_api_key, temperature=0)
    
    # ë™ì ìœ¼ë¡œ DataFrame ë³€ìˆ˜ì™€ íŒŒì¼ ì´ë¦„ì„ ë§¤í•‘í•˜ëŠ” ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    df_info_str = "\n".join([f"- df{i+1}: '{filename}'" for i, filename in enumerate(df_map.keys())])

    context_injection_prompt = ""
    if store_id:
        context_injection_prompt =  f"""
**[í˜„ì¬ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸]**
- ë‹¹ì‹ ì€ ì§€ê¸ˆ ê°€ë§¹ì  IDê°€ '{store_id}'ì¸ íŠ¹ì • ê°€ë§¹ì ì— ëŒ€í•´ ì»¨ì„¤íŒ…í•˜ê³  ìˆìŠµë‹ˆë‹¤.
- ì‚¬ìš©ìê°€ 'ìš°ë¦¬ ê°€ê²Œ', 'í•´ë‹¹ ë§¤ì¥' ë“± ìì‹ ì„ ì§€ì¹­í•˜ë©´, ì´ëŠ” ID '{store_id}'ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
- 'ENCODED_MCT' ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì´ ê°€ë§¹ì ì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë‹¤ë¥¸ ê°€ë§¹ì ê³¼ì˜ ë¹„êµ ë¶„ì„ ìš”ì²­ì´ ìˆì„ ìˆ˜ ìˆìœ¼ë‹ˆ, ì „ì²´ ë°ì´í„°ëŠ” í•„í„°ë§í•˜ì§€ ë§ê³  ì‚¬ìš©í•˜ì„¸ìš”.
"""

    # ì‚¬ìš©ìë‹˜ì˜ ì§€ì¹¨ê³¼ ì €ì˜ ë³´ì™„ ì‚¬í•­ì„ ê²°í•©í•œ ìµœì¢… prefix
    agent_prefix = f"""
ë‹¹ì‹ ì€ Python Pandas ì „ë¬¸ê°€ì´ë©°, ì£¼ì–´ì§„ DataFrame(df1, df2, ...)ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¶„ì„ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì„ë¬´ë¥¼ ë§¡ì•˜ìŠµë‹ˆë‹¤.

**[ì¤‘ìš” ì§€ì¹¨]**
1.  **ì»¨í…ìŠ¤íŠ¸ ì¸ì§€:** í˜„ì¬ ê°€ë§¹ì  ID '{store_id}'ì— ëŒ€í•œ ë¶„ì„ ìš”ì²­ì„ì„ ì¸ì§€í•˜ê³ , 'ìš°ë¦¬ ê°€ê²Œ' ë“±ì˜ í‘œí˜„ì€ ì´ IDë¥¼ ì˜ë¯¸í•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ì„í•˜ì„¸ìš”. 'ENCODED_MCT' ì»¬ëŸ¼ì„ í™œìš©í•˜ì„¸ìš”.
2.  **ììœ ë¡œìš´ ë¶„ì„:** ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ í•„ìš”í•œ ëª¨ë“  Python ì½”ë“œë¥¼ ììœ ë¡­ê²Œ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”. ì—¬ëŸ¬ ë‹¨ê³„ì˜ `Action`ì„ ì‚¬ìš©í•´ë„ ì¢‹ìŠµë‹ˆë‹¤.
3.  **ë°ì´í„° í•œê³„ ëª…ì‹œ:** ë¶„ì„ì— í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ë‹¤ë©´, ê·¸ ì‚¬ì‹¤ì„ ìµœì¢… ë‹µë³€ì— ëª…í™•íˆ í¬í•¨ì‹œí‚¤ì„¸ìš”.
4.  **ìµœì¢… ë³´ê³ :** ëª¨ë“  ë¶„ì„ì´ ëë‚˜ë©´, ë°˜ë“œì‹œ `Final Answer:` í‚¤ì›Œë“œë¡œ ì‹œì‘í•˜ëŠ” ìµœì¢… ìš”ì•½ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì—¬ ì‘ì—…ì„ ë§ˆë¬´ë¦¬í•˜ì„¸ìš”.


**[ì‚¬ìš© ê°€ëŠ¥í•œ DataFrame ì •ë³´]**
ë‹¹ì‹ ì—ê²ŒëŠ” ë‹¤ìŒê³¼ ê°™ì€ íŒŒì¼ë“¤ì´ DataFrameìœ¼ë¡œ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤. ì½”ë“œë¥¼ ì‘ì„±í•  ë•Œ ì´ ì •ë³´ë¥¼ ë°˜ë“œì‹œ ì°¸ê³ í•˜ì—¬ ì˜¬ë°”ë¥¸ ë³€ìˆ˜(df1, df2 ë“±)ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
{df_info_str}

**[ë¶„ì„ íŒŒì¼ ì„¤ëª…ì„œ]**
- 'big_data_set1_f.csv'ì˜ ì»¬ëŸ¼ ì„¤ëª…ì€ '2025_ë¹…ì½˜í…ŒìŠ¤íŠ¸_ë°ì´í„°_ë ˆì´ì•„ì›ƒ_20250902_ë°ì´í„°ì…‹1.csv' íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”.
- 'big_data_set2_f.csv'ì˜ ì»¬ëŸ¼ ì„¤ëª…ì€ '2025_ë¹…ì½˜í…ŒìŠ¤íŠ¸_ë°ì´í„°_ë ˆì´ì•„ì›ƒ_20250902_ë°ì´í„°ì…‹2.csv' íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”.
- 'big_data_set3_f.csv'ì˜ ì»¬ëŸ¼ ì„¤ëª…ì€ '2025_ë¹…ì½˜í…ŒìŠ¤íŠ¸_ë°ì´í„°_ë ˆì´ì•„ì›ƒ_20250902_ë°ì´í„°ì…‹3.csv' íŒŒì¼ì„ ì°¸ê³ í•˜ì„¸ìš”.
"""

    try:
        pandas_agent = create_pandas_dataframe_agent(
            llm, 
            dataframes,
            prefix=agent_prefix,
            agent_executor_kwargs={"handle_parsing_errors": True}, 
            verbose=True,
            allow_dangerous_code=True
        )
        
        response = pandas_agent.invoke({"input": query})
        return response.get("output", "ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except Exception as e:
        return create_tool_error("data_analyzer", e)
    
