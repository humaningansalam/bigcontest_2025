# src/core/graph_builder.py

import json
from langchain_core.messages import AIMessage
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.graph import StateGraph, END
from src.core.state import AgentState
import streamlit as st
import os
from langgraph.checkpoint.memory import MemorySaver

from src.core.common_tools.marketing_idea_tool import marketing_idea_generator_tool
from src.core.common_tools.rag_search_tool import rag_search_tool
from src.features.profile_management.tool import get_profile, update_profile
from src.features.data_analysis.tool import data_analysis_tool
from src.features.action_card_generation.tool import generate_action_card
from src.features.video_recommendation.tool import video_recommender_tool
from src.features.policy_recommendation.tool import policy_recommender_tool
from src.services.data_service import data_service
from src.core.intent_classifier import classify_intent
from .planner_prompt import build_planner_prompt
from src.utils.errors import create_tool_error
from src.config import PRIMARY_MODEL_NAME


    
load_dotenv()

google_api_key = st.secrets.get("GOOGLE_API_KEY", os.getenv("GOOGLE_API_KEY"))

llm = ChatGoogleGenerativeAI(model=PRIMARY_MODEL_NAME, google_api_key=google_api_key, temperature=0)

tools = {
    "data_analyzer": data_analysis_tool,
    "action_card_generator": generate_action_card,
    "marketing_idea_generator": marketing_idea_generator_tool,
    "get_profile": get_profile,
    "update_profile": update_profile,
    "rag_searcher": rag_search_tool,
    "video_recommender": video_recommender_tool,
    "policy_recommender": policy_recommender_tool,
}

#  ë‹¨ìˆœ ì‘ë‹µì„ ìœ„í•œ ë…¸ë“œ 
def simple_responder_node(state: AgentState):
    """
    'greeting', 'unknown' ë“± ê°„ë‹¨í•œ ì˜ë„ì— ëŒ€í•´ LLM í˜¸ì¶œ ì—†ì´ ì¦‰ì‹œ ë‹µë³€í•˜ëŠ” ê²½ëŸ‰ ë…¸ë“œ.
    """
    print("--- ğŸ‘‹ Simple Responder í™œë™ ì‹œì‘ ---")
    user_query = state['messages'][-1].content
    intent = classify_intent(user_query) # ì˜ë„ë¥¼ ë‹¤ì‹œ í•œë²ˆ í™•ì¸
    
    if intent == 'greeting':
        response_content = "ì•ˆë…•í•˜ì„¸ìš”, ì‚¬ì¥ë‹˜! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
    else: # 'unknown' ë˜ëŠ” ê¸°íƒ€ ì²˜ë¦¬ ë¶ˆê°€ëŠ¥í•œ ì˜ë„
        response_content = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 'ìš°ë¦¬ ê°€ê²Œ ì¬ë°©ë¬¸ìœ¨ ë¶„ì„í•´ì¤˜'ì™€ ê°™ì´ êµ¬ì²´ì ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì‹œê² ì–´ìš”?"
        
    # ìµœì¢… ìƒíƒœì— ë©”ì‹œì§€ë¥¼ ì¶”ê°€í•˜ì—¬ ë°˜í™˜
    return {"messages": [AIMessage(content=response_content)]}

# --- Router Node ---
def router_node(state: AgentState) -> dict:
    user_query = state['messages'][-1].content
    intent = classify_intent(user_query)
    print(f"--- ğŸš¦ ë¶„ì„ëœ ì˜ë„: {intent} ---")
    
    # "bigcon_request" ì˜ë„ì¼ ê²½ìš°, Plannerë¥¼ ê±´ë„ˆë›°ê³  Executorì—ê²Œ ì§ì ‘ ì§€ì‹œ
    if intent == "bigcon_request":
        print("--- [Router] Agent2 ì§ì ‘ í˜¸ì¶œ ê²°ì • ---")
        plan = [f"[Tool: action_card_generator] {user_query}"]
        return {
            "next_node": "executor", 
            "plan": plan,
            "past_steps": []
        }
    
    # ë™ì˜ìƒ ì¶”ì²œ
    elif intent == "video_recommendation":
        print("--- [Router] ë™ì˜ìƒ ì¶”ì²œ ë„êµ¬ ì§ì ‘ í˜¸ì¶œ ê²°ì • ---")
        plan = [f"[Tool: video_recommender] {user_query}"]
        return {"next_node": "executor", "plan": plan, "past_steps": []}

    # ì •ì±… ì¶”ì²œ
    elif intent == "policy_recommendation":
        print("--- [Router] ì§€ì›ì‚¬ì—… ì¶”ì²œ ë„êµ¬ ì§ì ‘ í˜¸ì¶œ ê²°ì • ---")
        plan = [f"[Tool: policy_recommender] {user_query}"]
        return {"next_node": "executor", "plan": plan, "past_steps": []}

    # ë³µí•©ì ì¸ ë¶„ì„ì´ í•„ìš”í•  ë•Œë§Œ Planner í˜¸ì¶œ
    elif intent in ["data_analysis", "marketing_idea", "rag_search"]:
        print("--- [Router] Planner í˜¸ì¶œ ê²°ì • ---")
        return {"next_node": "planner"}
        
    # í”„ë¡œí•„ ì¡°íšŒ
    elif intent == "profile_query":
        print("--- [Router] Synthesizer ì§ì ‘ í˜¸ì¶œ ê²°ì • (í”„ë¡œí•„ ê¸°ë°˜ ë‹µë³€) ---")
        return {"next_node": "synthesizer", "plan": []}
        
    # ì¸ì‚¬ë‚˜ ì•Œ ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì€ ìƒˆë¡œìš´ 'simple_responder'ê°€ ì²˜ë¦¬í•˜ë„ë¡ í•©ë‹ˆë‹¤.
    else: # greeting, unknown
        print("--- [Router] ë‹¨ìˆœ ì‘ë‹µ ë…¸ë“œ(Simple Responder) ì§ì ‘ í˜¸ì¶œ ê²°ì • ---")
        return {"next_node": "simple_responder"}

# --- Planner Node ---
def planner_node(state: AgentState):
    print("--- ğŸ¤” Planner í™œë™ ì‹œì‘ (ê°•í™” ë²„ì „) ---")
    
    prompt = build_planner_prompt(state)
    
    response = llm.invoke(prompt)
    plan = [step.strip() for step in response.content.split('\n') if step.strip() and '[Tool:' in step]
    
    print(f"--- ğŸ“ ìˆ˜ë¦½ëœ ê³„íš ---\n" + "\n".join(plan))
    return {"plan": plan, "past_steps": []}

# --- Executor Node ---
def executor_node(state: AgentState):
    print("--- âš™ï¸ Executor í™œë™ ì‹œì‘ (Phase 2 ìˆ˜ì • ë²„ì „) ---")
    
    if not state.get("plan"):
        return {}

    step = state["plan"][0]
    
    try:
        tool_name = step.split("[Tool: ")[1].split("]")[0]
        query = step.split("]")[1].strip()
    except IndexError:
        error_result = "ì˜¤ë¥˜: ê³„íšì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. [Tool: ë„êµ¬ì´ë¦„] í˜•ì‹ìœ¼ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤."
        return {"plan": state["plan"][1:], "past_steps": state.get("past_steps", []) + [(step, error_result)]}

    print(f"--- [ì‹¤í–‰] ë„êµ¬: {tool_name} // ì§€ì‹œì‚¬í•­: {query} ---")

    past_step_result = ""
    if tool_name in tools:
        tool = tools[tool_name]
        try:
            # [ìµœì¢… ìˆ˜ì •] ê° ë„êµ¬ì˜ Pydantic ëª¨ë¸ì— ë§ëŠ” ì •í™•í•œ ì¸ì ë”•ì…”ë„ˆë¦¬ ìƒì„±
            invoke_args = {}
            user_input_query = state["messages"][-1].content # ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸

            if tool_name == "data_analyzer":
                invoke_args = {"query": query, "store_id": state.get("current_profile", {}).get("profile_id")}
            
            elif tool_name == "action_card_generator":
                # ì´ ë„êµ¬ëŠ” Plannerì˜ ì§€ì‹œì‚¬í•­(query)ì´ ì•„ë‹Œ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸(user_input_query)ì„ ë°›ë„ë¡ ì„¤ê³„í–ˆì—ˆìŒ
                invoke_args = {"user_query": user_input_query, "profile": state.get("current_profile")}

            elif tool_name == "video_recommender":
                # ì´ ë„êµ¬ë„ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œì¸í™” ì¶”ì²œ
                invoke_args = {"user_query": user_input_query, "profile": state.get("current_profile")}

            elif tool_name == "marketing_idea_generator":
                # ì´ ë„êµ¬ëŠ” Plannerê°€ ì •ì œí•œ ì§€ì‹œì‚¬í•­(query)ì„ ì£¼ì œ(topic)ë¡œ ë°›ìŒ
                invoke_args = {"topic": query}

            elif tool_name == "policy_recommender":
                invoke_args = {"user_query": state["messages"][-1].content, "profile": state.get("current_profile")}

            else: # web_searcher, rag_search_tool ë“± 'query' ì¸ìë§Œ ë°›ëŠ” ê¸°ë³¸ ë„êµ¬ë“¤
                invoke_args = {"query": query}
            
            result = tool.invoke(invoke_args)
            past_step_result = str(result)

            # Pandas Agent ë˜ëŠ” Action Card Generatorê°€ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ë©´ ë°”ë¡œ ì¢…ë£Œ
            if (tool_name == "data_analyzer" and "Final Answer:" in past_step_result) or \
               (tool_name == "action_card_generator") or \
               (tool_name == "video_recommender") or \
               (tool_name == "policy_recommender"):
                
                final_content = past_step_result
                if "Final Answer:" in past_step_result:
                    final_content = past_step_result.split("Final Answer:", 1)[1].strip()

                return {
                    "plan": [],
                    "past_steps": state.get("past_steps", []) + [(step, past_step_result)],
                    "messages": state.get("messages", []) + [AIMessage(content=final_content)],
                    "is_final_answer": True
                }

        except Exception as e:
            print(f"--- ğŸš¨ EXECUTORê°€ ë„êµ¬ í˜¸ì¶œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e} ---")
            past_step_result = create_tool_error(tool_name, e)
    else:
        past_step_result = f"ì˜¤ë¥˜: '{tool_name}'ì€(ëŠ”) ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ì…ë‹ˆë‹¤."

    return {"plan": state["plan"][1:], "past_steps": state.get("past_steps", []) + [(step, past_step_result)]}

# --- Synthesizer Node (RAG-Aware) ---
def synthesizer_node(state: AgentState):
    print("--- âœï¸ Synthesizer ìµœì¢… ë‹µë³€ ì‘ì„± (ë…¼ë¦¬ ê°•í™” ë²„ì „) ---")
    
    user_query = state['messages'][-1].content
    profile_json_str = json.dumps(state.get('current_profile'), ensure_ascii=False, indent=2)
    
    # Toolì„ ì‚¬ìš©í•œ ê²½ìš°
    if state.get("past_steps"):
        evidence = "\n\n".join(
            [f"**ì‹¤í–‰ ë‚´ìš©:** {step}\n**ê²°ê³¼:**\n{result}" for step, result in state.get("past_steps")]
        )
        base_context = f"**[ìˆ˜ì§‘ëœ ê·¼ê±° ìë£Œ]**\n{evidence}\n\n**[ì°¸ê³ : ê°€ë§¹ì  í”„ë¡œí•„]**\n{profile_json_str}"
    
    # Toolì„ ì‚¬ìš©í•˜ì§€ ì•Šì€ ê²½ìš° 
    else:
        prompt_check = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì£¼ì–´ì§„ í”„ë¡œí•„ ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œì§€ 'yes' ë˜ëŠ” 'no'ë¡œë§Œ ë‹µí•´ì£¼ì„¸ìš”.
        
        [í”„ë¡œí•„ ì •ë³´]
        {data_service.get_summary_for_planner(state['current_profile']['profile_id'])}
        
        [ì‚¬ìš©ì ì§ˆë¬¸]
        "{user_query}"
        
        ë‹µë³€ (yes/no):"""
        
        is_profile_sufficient = llm.invoke(prompt_check).content.strip().lower()

        if 'yes' in is_profile_sufficient:
            print("--- [Synthesizer] í”„ë¡œí•„ ì •ë³´ë§Œìœ¼ë¡œ ë‹µë³€ ê°€ëŠ¥. RAG ê²€ìƒ‰ ìƒëµ. ---")
            base_context = f"**[ê°€ë§¹ì  í”„ë¡œí•„ ì •ë³´]**\n{profile_json_str}"
        else:
            print("--- [Synthesizer] ì™¸ë¶€ ì •ë³´ í•„ìš”. RAG ê²€ìƒ‰ ìˆ˜í–‰. ---")
            rag_context = data_service.search_for_context(query=user_query)
            base_context = f"**[ì°¸ê³  ìë£Œ]**\n{rag_context}\n\n**[ê°€ë§¹ì  í”„ë¡œí•„ ì •ë³´]**\n{profile_json_str}"

    prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤. ì•„ë˜ [ì‚¬ìš©ì ì§ˆë¬¸]ì— ëŒ€í•´, ì£¼ì–´ì§„ [í•µì‹¬ ê·¼ê±°]ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì¹œì ˆí•˜ê³  ëª…í™•í•˜ê²Œ ìµœì¢… ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë§Œì•½ [ì°¸ê³  ìë£Œ]ê°€ ìˆë‹¤ë©´, í•´ë‹¹ ë‚´ìš©ì„ ì¸ìš©í•˜ì—¬ ë‹µë³€ì˜ ì‹ ë¢°ë„ë¥¼ ë†’ì—¬ì£¼ì„¸ìš”.

**[ì‚¬ìš©ì ì§ˆë¬¸]**
{user_query}

**[í•µì‹¬ ê·¼ê±°]**
{base_context}

**[ìµœì¢… ë‹µë³€]**
"""
    response = llm.invoke(prompt)
    return {"messages": [AIMessage(content=response.content)]}




# --- ìµœì¢… ê·¸ë˜í”„ êµ¬ì„± ---
workflow = StateGraph(AgentState)

# 1. ëª¨ë“  ë…¸ë“œë¥¼ ê·¸ë˜í”„ì— ì¶”ê°€í•©ë‹ˆë‹¤.
workflow.add_node("router", router_node)
workflow.add_node("planner", planner_node)
workflow.add_node("executor", executor_node)
workflow.add_node("synthesizer", synthesizer_node)
workflow.add_node("simple_responder", simple_responder_node)

# 2. ê·¸ë˜í”„ì˜ ì‹œì‘ì ì„ 'router'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.
workflow.set_entry_point("router")

# 3. ê° ë…¸ë“œë¥¼ ìˆœì„œëŒ€ë¡œ ì—°ê²°í•©ë‹ˆë‹¤.

# 'router'ì˜ ê²°ì •ì— ë”°ë¼ 'planner' ë˜ëŠ” 'synthesizer'ë¡œ ë¶„ê¸°í•©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "router",
    lambda state: state.get("next_node"),
    {
        "planner": "planner",
        "executor": "executor", 
        "synthesizer": "synthesizer",
        "simple_responder": "simple_responder"
    }
)

# 'planner'ëŠ” í•­ìƒ 'executor'ë¡œ ì—°ê²°ë©ë‹ˆë‹¤.
workflow.add_edge("planner", "executor")

# ì´ í•¨ìˆ˜ê°€ executor ë…¸ë“œ ë‹¤ìŒì— ì–´ë””ë¡œ ê°ˆì§€ ê²°ì •í•©ë‹ˆë‹¤.
def after_executor_logic(state: AgentState):
    # 1ìˆœìœ„: íŠ¹ë³„ ì „ë¬¸ê°€(data_analyzer, action_card_generator)ê°€ ìµœì¢… ë‹µë³€ì„ ë§Œë“¤ì—ˆëŠ”ê°€?
    if state.get("is_final_answer"):
        # ê·¸ë ‡ë‹¤ë©´ ì¦‰ì‹œ ì¢…ë£Œ(END)í•˜ë¼ëŠ” ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
        return "end" 
    
    # 2ìˆœìœ„: ë‚¨ì€ ê³„íšì´ ìˆëŠ”ê°€?
    elif state.get("plan"):
        # ê·¸ë ‡ë‹¤ë©´ executorë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë¼ëŠ” ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
        return "continue"
        
    # 3ìˆœìœ„: ëª¨ë“  ê³„íšì´ ëë‚¬ê³ , ìµœì¢… ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ëŠ”ê°€?
    else:
        # ê·¸ë ‡ë‹¤ë©´ synthesizerë¥¼ ì‹¤í–‰í•˜ë¼ëŠ” ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤.
        return "synthesize"

# ìœ„ í•¨ìˆ˜ê°€ ë°˜í™˜í•˜ëŠ” ì‹ í˜¸('end', 'continue', 'synthesize')ì— ë”°ë¼
# ë‹¤ìŒì— ì‹¤í–‰í•  ë…¸ë“œë¥¼ ë§¤í•‘í•©ë‹ˆë‹¤.
workflow.add_conditional_edges(
    "executor",
    after_executor_logic,
    {
        "continue": "executor",
        "synthesize": "synthesizer",
        "end": END 
    }
)

# 'synthesizer'ëŠ” í•­ìƒ ë§ˆì§€ë§‰ì´ë©°, ê·¸ë˜í”„ë¥¼ ì¢…ë£Œ(END)í•©ë‹ˆë‹¤.
workflow.add_edge("synthesizer", END)

# 4. ê·¸ë˜í”„ë¥¼ ì»´íŒŒì¼í•©ë‹ˆë‹¤.
memory = MemorySaver()
graph = workflow.compile(checkpointer=memory)

